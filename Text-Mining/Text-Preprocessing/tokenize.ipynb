{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 토큰화(Tokenize)\n",
    "\n",
    "- 문장 토큰화\n",
    "    - 문서에서 문장을 분리\n",
    "    - 문장의 마침표, 개행문자 등을 기준으로 분리\n",
    "    - NLTK의 sent_tokenize() 함수\n",
    "    \n",
    "    \n",
    "- 단어 토큰화\n",
    "    - 문장에서 단어를 분리\n",
    "    - 공백, 콤마, 마침표, 개행문자 등을 기준으로 단어 분리\n",
    "    - NLTK의 word_tokenize() 함수"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 문장 토큰화"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'list'> 3\n['The Matirx is eveywhere its all around us, herer event in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "text_sample = 'The Matirx is eveywhere its all around us, herer event in this room. \\\n",
    "    You can see it out your window or on your television. \\\n",
    "        You feel it when you go to work, or go to church or pay your taxes.'\n",
    "\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "source": [
    "## 단어 토큰화"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'list'> 15\n['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentences = 'The Matrix is everywhere its all around us, here even in this room.'\n",
    "words = word_tokenize(sentences)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'list'> 3\n[['The', 'Matirx', 'is', 'eveywhere', 'its', 'all', 'around', 'us', ',', 'herer', 'event', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ]
  }
 ]
}